{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clustering_methods.ipynb","provenance":[],"collapsed_sections":["1BOZpIUSANBT"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"RPT9JD5UZiu0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592690477122,"user_tz":-180,"elapsed":4241,"user":{"displayName":"Диана Сергеевна Киселева","photoUrl":"","userId":"11728252636771326870"}},"outputId":"91398031-e472-45e9-c695-a1c88d13ea30"},"source":["import math\n","import numpy as np\n","\n","import colorsys\n","from matplotlib import pyplot as plt\n","from matplotlib import cm, colors, colorbar\n","\n","from tqdm import tqdm\n","\n","import cv2\n","\n","from sklearn.preprocessing import normalize \n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import VarianceThreshold\n","\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.manifold import Isomap\n","from sklearn.manifold import TSNE\n","\n","import pickle\n","from sklearn.cluster import OPTICS, cluster_optics_dbscan\n","\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping\n","from keras.models import load_model"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4FESY5xeVNwA","colab_type":"code","colab":{}},"source":["way = '/content/drive/My Drive/Colab Notebooks/' # путь для сохранения обрабатываемых массивов и моделей"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BresNpRI7fkw","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4vvsT2Z7gN0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592690517618,"user_tz":-180,"elapsed":2249,"user":{"displayName":"Диана Сергеевна Киселева","photoUrl":"","userId":"11728252636771326870"}},"outputId":"214c6b83-f539-449d-dca0-18ace4fb7446"},"source":["cd /content/drive/My Drive/Colab Notebooks"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1BOZpIUSANBT","colab_type":"text"},"source":["##Методы"]},{"cell_type":"code","metadata":{"id":"3TWbVRP9AT4u","colab_type":"code","colab":{}},"source":["def visualize_ampField(ampField, ind):\n","# визуализирует первые 1000 трасс набора данных с номером ind (разные сейсмические данные)\n","\n","    if (ind > ampField.shape[0] - 1):\n","        print('There is no dataset with this number')\n","    current_ampField = ampField[ind]\n","    \n","    rows = current_ampField[:1000].shape[0]\n","    cols = current_ampField[:1000].shape[1]\n","    fig, axes = plt.subplots(1, 1, figsize=(rows/30, cols/30))\n","    \n","    fig.suptitle('dataset ' + str(ind), fontsize=20, x=0.1, y=0.9)\n","    \n","    normMax = max([-current_ampField.min(), current_ampField.max()])\n","    norm = colors.Normalize(vmin=-normMax, vmax=normMax)\n","    cmap = cm.seismic\n","    \n","    axes.imshow(current_ampField[:1000].T, cmap=\"seismic\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4zsyW8OBoMW","colab_type":"code","colab":{}},"source":["def create_objects(var_name, ampField, num_traces, num_samples):\n","# вырезает объекты задаваемого разреза из поля амплитуд\n","# var_name - имя для сохранения\n","# ampField - поле амплитуд всех наборов данных (состоит из 2d массивов разного размера (разные сейсмические данные))\n","# num_traces - число трасс в каждом объекте\n","# num_samples - число дискретов в каждом объекте\n","\n","    def obj_for_one_dataset(ampField, num_traces, num_samples):\n","    # вырезает объекты из сейсмических данных одного набора\n","\n","        # размер, который будет кратен подаваемым размерам одного объекта\n","        obj_traces = math.floor(ampField.shape[0] / num_traces) * num_traces\n","        obj_samples = math.floor(ampField.shape[1] / num_samples) * num_samples\n","    \n","        # число получаемых объектов\n","        elem_traces = math.floor(obj_traces / num_traces)\n","        elem_samples = math.floor(obj_samples / num_samples)\n","        num_of_elem = elem_traces * elem_samples\n","        print(\"Количество элементов по трассам {0}, по дискретам {1}, всего {2}\\n\".format(elem_traces, elem_samples, num_of_elem))\n","    \n","        ampField_divisible = np.copy(ampField[:obj_traces, :obj_samples])\n","        ampField_div_sp = np.array(np.split(ampField_divisible, elem_traces, axis = 0))\n","\n","        objects = np.array(np.split(ampField_div_sp[0], elem_samples, axis = 1))\n","        for part in ampField_div_sp[1:]:\n","            part1 = np.array(np.split(part, elem_samples, axis = 1))\n","            objects = np.concatenate((objects, part1))\n","        return objects\n","    \n","    # проходим по всем наборам данных\n","    objects = obj_for_one_dataset(ampField[0], num_traces, num_samples)\n","    for A in ampField[1:]:\n","        objects = np.concatenate((objects, obj_for_one_dataset(A, num_traces, num_samples)), axis = 0)\n","    \n","    name_file = \"{0}__tr={1}_smpl={2}.npy\".format(var_name, num_traces, num_samples)\n","    np.save(way + name_file, objects)\n","    \n","    return objects"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"He8rCGEkBsuS","colab_type":"code","colab":{}},"source":["def visualize_objects(data, random=False, title='Objects'):\n","# визуализирует все подаваемые объекты при random=False\n","# при random=True визуализирует 49 случайных\n","# title - заголовок\n","\n","    if random == True:\n","\n","        ind_for_imshow = np.random.randint(0, data.shape[0], size=49)\n","        obj_for_imshow = data[ind_for_imshow]\n","\n","        nrows = 7\n","        ncols = 7\n","\n","        im_height = data.shape[2]\n","        im_width = data.shape[1]\n","    \n","        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(im_width * ncols/50, im_height * nrows/50))\n","\n","        for i, ax in enumerate(axes.flat):\n","        \n","          normMax = max([-obj_for_imshow[i].min(), obj_for_imshow[i].max()])\n","          norm = colors.Normalize(vmin=-normMax, vmax=normMax)\n","          cmap = cm.seismic\n","        \n","          ax.axis('off')\n","        \n","          ax.imshow(obj_for_imshow[i].T, norm=norm, cmap=cmap)\n","        \n","        return\n","      \n","    if data.shape[0] == 1 or len(data.shape) == 2:\n","         \n","        plt.axis('off')\n","        \n","        if data.shape[0] == 1:\n","            normMax = max([-data[0].min(), data[0].max()])\n","            norm = colors.Normalize(vmin=-normMax, vmax=normMax)\n","            cmap = cm.seismic\n","          \n","            plt.imshow(data[0].T, norm=norm, cmap=cmap)\n","        else:\n","            if len(data.shape) == 2:\n","                normMax = max([-data.min(), data.max()])\n","                norm = colors.Normalize(vmin=-normMax, vmax=normMax)\n","                cmap = cm.seismic\n","\n","                plt.imshow(data.T, norm=norm, cmap=cmap)\n","        return\n","        \n","    num_obj = data.shape[0]\n","\n","    nrows = int(num_obj**0.5)\n","    while num_obj % nrows != 0:\n","        nrows += 1\n","    ncols = int(num_obj / nrows)\n","    \n","    #if ncols == 1 or nrows == 1: # для более удобного отображения\n","    #  num_obj = num_obj - 1\n","    #  nrows = 2\n","    #  ncols = int(num_obj / nrows)\n","\n","    im_height = data.shape[2]\n","    im_width = data.shape[1]\n","    \n","    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(im_width * ncols/50, im_height * nrows/50))\n","    \n","    fig.suptitle(title, fontsize=20, x=0.1, y=0.9)\n","    \n","    for i, ax in enumerate(axes.flat):\n","        \n","        normMax = max([-data[i].min(), data[i].max()])\n","        norm = colors.Normalize(vmin=-normMax, vmax=normMax)\n","        cmap = cm.seismic\n","        \n","        ax.axis('off')\n","        \n","        ax.imshow(data[i].T, norm=norm, cmap=cmap)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3G7SBlxCCLza","colab_type":"code","colab":{}},"source":["def create_features1(var_name, objects, samples_step = 2, draw = False, save = True):\n","# для каждого объекта сигнальные (усредняем) и CV-атрибуты\n","# var_name - для сохранения\n","# из массива объектов получает массив признаков\n","# samples_step - время одного дискрета в мс\n","# draw, save - параметры для визуализации результатов вычисления\n","\n","    def feat_of_one_obj(obj):\n","    # вычисляет признаки для одного объекта\n","\n","        names_attrs = np.array([\"средняя амплитуда\", \"среднеквадратичная амплитуда\", \\\n","        \"частота максимума спектра\", \"энергия спектра\", \"ширина спектра\", \"центральная частота\", \"доля важной энергии спектра\", \\\n","        \"временная разреженность\", \"динамическая выраженность\", \"индекс полосы пропускания\", \"доминантная частота\", \\\n","        \"максимальная длина линий Хафа\", \"средняя длина линий Хафа\", \\\n","        \"среднее угла линии Хафа (обратная величина)\", \"взвешенное среднее угла линии Хафа (обратная величина)\", \"стандартное отклонение угла линии Хафа (обратная величина)\", \\\n","        \"максимальная длина контура\", \"средняя длина контура\", \"максимальная спрямлённость\", \"средняя спрямлённость\"])\n","\n","        for_obj_attrs = dict.fromkeys(names_attrs, 0)\n","\n","        num_of_traces_in_objent = obj.shape[0]\n","        num_of_samples_in_objent = obj.shape[1]\n","\n","        # АМПЛИТУДНЫЕ\n","        for_obj_attrs[\"средняя амплитуда\"] = round(np.abs(obj).sum()/num_of_traces_in_objent/num_of_samples_in_objent, 5)\n","        for_obj_attrs[\"среднеквадратичная амплитуда\"] = round((obj**2).sum()**(0.5)/num_of_traces_in_objent/num_of_samples_in_objent, 6)\n","\n","        # СПЕКТРАЛЬНЫЕ\n","        # преобразование Фурье - линейная операция, поэтому можем взять Фурье от средней трассы\n","        # (в отличие от АКФ, где надо считать среднее от АКФ каждой трассы)\n","        m_trace = obj.sum(axis = 0)/num_of_traces_in_objent\n","        fourier = np.fft.fft(m_trace)\n","        fourier = fourier[int(fourier.size/2):]\n","        fourier = fourier[::-1]\n","\n","        f_discr = 1/(samples_step/1000) # частота дискретизации (не забыли, что samples_step в мс)\n","        df = f_discr/num_of_samples_in_objent # шаг по частоте (ДПФ: число дискретов образа = числу дискретов прообраза)\n","        faxis = np.arange(0, int(round(f_discr/2)), df) # f_discr/2 - частота Найквиста\n","        spectra = 2/(f_discr*num_of_samples_in_objent) * (abs(fourier))**2 # нормируем квадрат амплитудного спектра\n","\n","        for_obj_attrs[\"частота максимума спектра\"] = round(spectra.argmax() * df, 5)\n","        for_obj_attrs[\"энергия спектра\"] = np.trapz(spectra[:spectra.size], x=faxis)\n","\n","        if spectra.max() != 0:\n","            for_obj_attrs[\"ширина спектра\"] = round(for_obj_attrs[\"энергия спектра\"]/spectra.max(), 5)\n","\n","        # Здесь считаем центральную частоту по критерию, что энергии от Fmax/2 до неё и от неё до 2*Fmax совпадают\n","        # или, в нашем случае, отличаются друг от друга не более, чем на 5% полной энергии\n","        Edelta = for_obj_attrs[\"энергия спектра\"]*0.05\n","\n","        Fc, Er, El = 0, 0, 0\n","        # Проходим от Fmax/2 до 2*Fmax, считая обе энергии на каждом шаге и сравнивая их\n","        for i in range(int(np.argmax(spectra)/2)+1, int(2*np.argmax(spectra))-1):\n","            Fc = i*df\n","            try:\n","                El = np.trapz(spectra[int(np.argmax(spectra)/2):i], x=np.arange(np.argmax(spectra)/2*df, Fc, df))\n","            except ValueError:\n","                El = (i - int(np.argmax(spectra)/2))*df*spectra.max()/2\n","            try:\n","                Er = np.trapz(spectra[i:int(round(2*np.argmax(spectra)))], x=np.arange(Fc+df/2, 2*np.argmax(spectra)*df, df))\n","            except ValueError:\n","                Er = (int(round(2*np.argmax(spectra))) - i)*df*spectra.max()/2\n","            if abs(Er - El) < Edelta:\n","                break\n","\n","        for_obj_attrs[\"центральная частота\"] = round(Fc, 5)\n","        for_obj_attrs[\"доля важной энергии спектра\"] = round((El + Er)/for_obj_attrs[\"энергия спектра\"], 5)\n","\n","        for_obj_attrs[\"энергия спектра\"] =round(for_obj_attrs[\"энергия спектра\"], 5) # в двух местах до этого используется: нет смысла округлять там\n","\n","        # КОРРЕЛЯЦИОННЫЕ\n","        acfField = np.array([])\n","        for trace in obj:\n","            temp = np.correlate(trace, trace, mode='full')\n","            temp = temp[int(temp.size/2):]\n","            acfField = np.append(acfField, temp)\n","        acfField = np.reshape(acfField, (num_of_traces_in_objent, num_of_samples_in_objent))\n","\n","        acf_params = np.array([])\n","        for acf_of_trace in acfField:\n","            acf_max = acf_of_trace.max()\n","            # нахождение индексов массива, после которых значение массива меняет знак\n","            acf_zeros = np.diff(np.sign(acf_of_trace))\n","            try:\n","                maxtomin_index = np.where(acf_zeros == -2)[0][0]\n","                mintomax_index = np.where(acf_zeros == 2)[0][0]\n","                acf_fmin = acf_of_trace[0 : mintomax_index + 1].min()\n","                ind_fmin = acf_of_trace[0 : mintomax_index + 1].argmin()\n","                x_fall1 = maxtomin_index\n","                y_fall1 = acf_of_trace[x_fall1]\n","                x_fall2 = maxtomin_index + 1\n","                \n","                #результат очень сильно зависит от способа интерполяции, поэтому вообще не будем интерполировать\n","                ind_fzero = x_fall2\n","                acf_params = np.append(acf_params, np.array([acf_max, -acf_fmin, ind_fmin, ind_fzero,2-ind_fmin/ind_fzero,1-(-acf_fmin)/acf_max,((2-ind_fmin/ind_fzero)**2+(1-(-acf_fmin)/acf_max)**2)**0.5,1/(4*ind_fzero*samples_step/1000)]))\n","            except IndexError:\n","                acf_params = np.append(acf_params, np.array([0,0,0,0,  -1,-1,-1,-1]))\n","\n","        acf_params = np.reshape(acf_params, (num_of_traces_in_objent, 8))\n","\n","        acf_params_mean = acf_params.sum(axis = 0)/num_of_traces_in_objent\n","\n","        for_obj_attrs[\"временная разреженность\"] = round(acf_params_mean[4], 5)\n","        for_obj_attrs[\"динамическая выраженность\"] = round(acf_params_mean[5], 5)\n","        for_obj_attrs[\"индекс полосы пропускания\"] = round(acf_params_mean[6], 5)\n","        for_obj_attrs[\"доминантная частота\"] = round(acf_params_mean[7], 5)\n","\n","        # ЗРЕНИЕ МАШИНЫ\n","        # делаем изображение серым (отображаем значения амплитуд в интервал 0-255, приводим полученный массив к типу byte)\n","        ampField_T = obj.T\n","        im = (np.flip(ampField_T, axis=0) - ampField_T.min())/(ampField_T.max() - ampField_T.min())*255\n","        im = 255 - im\n","        im = im.astype(np.uint8)\n","        im = np.flip(im)\n","\n","        # чуть-чуть заблюриваем картинку (чтобы убрать шум)\n","        blur = cv2.GaussianBlur(im, (3,3), 0)\n","\n","        # применяем пороговый фильтр\n","        # (алгоритм Отсу вычисляет оптимальное значение порога по точке, равноудалённой от двух пиков гистограммы,\n","        # соответствующих чёрному и белому цветам)\n","        ret, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n","\n","        # делаем скелетизацию, т.е. истончение белых участков изображения до линий толщиной в 1 пиксель\n","        thresh2 = thresh.copy()\n","        size = np.size(thresh2)\n","        skel = np.zeros(thresh.shape, np.uint8)\n","        element = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))\n","\n","        done = False\n","        while (not done):\n","            eroded = cv2.erode(thresh2, element)\n","            temp = cv2.dilate(eroded, element)\n","            temp = cv2.subtract(thresh2, temp)\n","            skel = cv2.bitwise_or(skel, temp)\n","            thresh2 = eroded.copy()\n","            zeros = size - cv2.countNonZero(thresh2)\n","            if zeros == size:\n","                done = True\n","\n","        # делаем преобразование Хафа (его оптимизированную версию Probabilistic Hough Transform), которое на выходе даёт нам все найденные прямолинейные отрезки\n","        # skel - подаваемое изображение\n","        # lines - туда запишутся тета и ро каждой задетектированной линии (пишет координаты начала и конца каждой линии)\n","        # 2ой аргумент - разрешение по ро в пикселях\n","        # 3ий - разрешение по тета в радианах\n","        # 4ый - порог детектирования (сколько пересечений синусоид достаточно для признания линии; количество точек в линии, чтобы та задетектировалась)\n","        # 5ый - минимальное количество точек, способных сформировать линию (только для HoughLinesP)\n","        # 6ой - максимальное расстояние между двумя точками, чтобы считать, что эти точки лежат на одной линии (только для HoughLinesP)\n","        linesP = cv2.HoughLinesP(skel, 30, 0.3*np.pi/180, 200, None, 2, 2)\n","        \n","        #if linesP is None:\n","        #    print(\"linesP is None\")\n","        #else:\n","        #    print(\"!!!!!!!!!!!!\")\n","        #plt_Hl = np.zeros(shape=[im.shape[0], im.shape[1], 3], dtype=np.uint8)\n","\n","        if draw == True:\n","          plt_Hl = np.zeros(shape=[im.shape[0], im.shape[1], 3], dtype=np.uint8) # picture for Hough lines\n","\n","        houghLenghts = []\n","        houghAngles = []\n","        # для каждого отрезка находим длину и угол и сразу рисуем их на выделенном для них фоне, цветом обозначаем угол\n","        # затем вычисляем среднюю и максимальную длину, а также разброс по углам\n","        if linesP is not None:\n","            for i in range(len(linesP)):\n","\n","                l = linesP[i][0] # координаты начала и конца очередной линии\n","                length = ((l[2]-l[0])**2 + (l[3]-l[1])**2)**0.5 # длина очередной линии\n","                houghLenghts.append(length)\n","\n","                angle = np.arctan((l[3]-l[1])/(l[2]-l[0]+0.0001)) # угол наклона очередной линии\n","                houghAngles.append(angle*180/math.pi) # в градусах\n","\n","                if draw == True:\n","                  #hsv_to_rgb переводит из HSV (координаты: цветовой тон от 0 до 360 градусов (поэтому и делим на math.pi), насыщенность и яркость) в RGB (красный, зеленый, синий)\n","                  #задаём цвет в HSV, потому что сам по себе цвет задаётся только первой координатой\n","                  color = colorsys.hsv_to_rgb((angle+math.pi/2)/math.pi, 1, 1)\n","                  color = tuple([255*x for x in color])\n","\n","                  #изображение, координаты отрезка, цвет, толщина, cv2.LINE_AA - чтобы отрезок был сглажен\n","                  cv2.line(plt_Hl, (l[0], l[1]), (l[2], l[3]), color, 1, cv2.LINE_AA) # система RGB (для matplotlib)\n","\n","            maxHLen = max(houghLenghts)\n","            avgHLen = np.average(houghLenghts)\n","            avgHAng = np.average(houghAngles)\n","            avgHAngW = np.average(houghAngles, weights=np.array(houghLenghts)/max(houghLenghts))\n","            stdHAng = np.std(houghAngles)\n","            if avgHAng != 0:\n","                avgHAng = 1 / avgHAng\n","            if avgHAngW != 0:\n","                avgHAngW = 1 / avgHAngW\n","            if stdHAng != 0:\n","                stdHAng = 1 / stdHAng\n","        else:\n","            maxHLen = 0\n","            avgHLen = 0\n","            avgHAng = 0\n","            avgHAngW = 0\n","            stdHAng = 0\n","\n","        for_obj_attrs[\"максимальная длина линий Хафа\"] = round(maxHLen, 5)\n","        for_obj_attrs[\"средняя длина линий Хафа\"] = round(avgHLen, 5)\n","        for_obj_attrs[\"среднее угла линии Хафа (обратная величина)\"] = round(avgHAng, 5)\n","        for_obj_attrs[\"взвешенное среднее угла линии Хафа (обратная величина)\"] = round(avgHAngW, 5)\n","        for_obj_attrs[\"стандартное отклонение угла линии Хафа (обратная величина)\"] = round(stdHAng, 5)\n","\n","        # ищем контуры\n","        contours, hierarchy = cv2.findContours(skel, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n","\n","        if draw == True:\n","          # создаются чёрные фоновые изображения, на которых будем рисовать контуры и линии\n","          plt_Tr = np.zeros(shape=[im.shape[0], im.shape[1], 3], dtype=np.uint8) # for Traceability\n","          plt_Str = plt_Tr.copy() # for Straightness\n","\n","        lengths = []            # Длины по всем контурам\n","        straightnesses = []     # Спрямлённости\n","        if contours is not None:\n","            for cnt in contours:\n","                # cv2.arcLength - длина подаваемой кривой, False - говорим, что наша кривая незамкнутая\n","                lengths.append(cv2.arcLength(cnt, False))\n","\n","                # cv2.boundingRect - the function calculates and returns the minimal up-right bounding rectangle for the specified point set\n","                cntRect = cv2.boundingRect(cnt)\n","                # спрямлённость: cntRect[3] - высота прямоугольника, описывающего контур, cntRect[2] - его ширина\n","                straightnesses.append(1 - cntRect[3]/cntRect[2])\n","\n","                if draw == True:\n","                  color1 = colorsys.hsv_to_rgb(0.75 - cntRect[3]/cntRect[2], 1, 1)\n","                  color = colorsys.hsv_to_rgb(cv2.arcLength(cnt, False)/num_of_traces_in_objent*2, 1, 1 - cntRect[3]/cntRect[2])\n","\n","                  color = tuple([255*x for x in color])\n","                  color1 = tuple([255*x for x in color1])\n","                  \n","                  cv2.drawContours(plt_Tr, [cnt], -1, color, 1) # система RGB (для matplotlib)\n","                  cv2.drawContours(plt_Str, [cnt], -1, color1, 1) # система RGB (для matplotlib)\n","\n","            maxCLen = max(lengths)\n","            avgCLen = np.average(lengths)\n","            maxStraightness = max(straightnesses)\n","            avgStraightness = np.average(straightnesses)\n","\n","        else:\n","            maxCLen = 0\n","            avgCLen = 0\n","            maxStraightness = 0\n","            avgStraightness = 0\n","\n","        for_obj_attrs[\"максимальная длина контура\"] = round(maxCLen, 5)\n","        for_obj_attrs[\"средняя длина контура\"] = round(avgCLen, 5)\n","        for_obj_attrs[\"максимальная спрямлённость\"] = round(maxStraightness, 5)\n","        for_obj_attrs[\"средняя спрямлённость\"] = round(avgStraightness, 5)\n","\n","        if draw == True:\n","          visualize_objects(obj)\n","          plt.show()\n","\n","          fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(6, 18))\n","          \n","          a1 = ax1.imshow(plt_Hl, cmap=\"seismic\") # цветом выделяется угол наклона\n","          ax1.set_title(\"Hough\")\n","          #fig.colorbar(a1, ax=ax1)\n","          a2 = ax2.imshow(plt_Tr, cmap=\"seismic\") # цветом выделяется длина контура\n","          ax2.set_title(\"Trans\")\n","          #fig.colorbar(fig, ax=ax2)\n","          a3 = ax3.imshow(plt_Str, cmap=\"seismic\") # цветом выделяется спрямленность контура\n","          ax3.set_title(\"Straight\")\n","          #fig.colorbar(fig, ax=ax3)\n","\n","          #norm1 = colors.Normalize(vmin=0, vmax=2000)\n","          #norm2 = colors.Normalize(vmin=0, vmax=180)\n","          #cmap2 = colors.LinearSegmentedColormap.from_list('mycmap', ['Cyan','Lime','Yellow','Red','Magenta','Blue'])\n","          #norm3 = colors.Normalize(vmin=0, vmax=1)\n","\n","          #Cb1 = colorbar.ColorbarBase(ax1, norm=norm1)\n","          #Cb2 = colorbar.ColorbarBase(ax2, cmap=cmap2, norm=norm2)\n","          #Cb3 = colorbar.ColorbarBase(ax3, norm=norm3)\n","\n","          plt.show()\n","\n","          print()\n","          for keys, values in for_obj_attrs.items():\n","            print(keys, ' = ', values)\n","          print()\n","\n","        return for_obj_attrs\n","    \n","    if len(objects.shape) == 2: # если подали только один объект\n","      attrs = np.array(list(feat_of_one_obj(objects).values()))\n","      attrs = attrs[np.newaxis, :]\n","      return attrs\n","\n","    attrs = np.array(list(feat_of_one_obj(objects[0]).values()))\n","    attrs = attrs[np.newaxis, :]\n","\n","    for elem in tqdm(objects[1:]):     \n","        attrs_for_iteration = np.array(list(feat_of_one_obj(elem).values()))\n","        attrs_for_iteration = attrs_for_iteration[np.newaxis, :]\n","        attrs = np.concatenate(( attrs, attrs_for_iteration ), axis = 0)\n","    \n","    if save == True:\n","      name_file = \"{0}__tr={1}_smpl={2}.npy\".format(var_name, objects[0].shape[0], objects[0].shape[1])\n","      np.save(way + name_file, attrs)\n","\n","    return attrs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFjjI1lsCUEu","colab_type":"code","colab":{}},"source":["def create_features2(var_name, objects, samples_step = 2, draw = False, save = True):\n","# для каждого объекта получает усреднённые сигнальные и CV-атрибуты, сигнальные для каждой трассы, сохраняет картины контуров (черно-белую и со спрямлённостью)\n","# var_name - для сохранения\n","# из массива объектов получает массив признаков\n","# samples_step - время одного дискрета в мс\n","# draw, save - параметры для визуализации результатов вычисления\n","# если надо подать один объект (например, второй по индексу), подавать в формате create_features2('your var_name', objects[2:3])\n","\n","  def feat_of_one_obj(obj):\n","  # вычисляет признаки для одного объекта\n","\n","          traces_in_object = obj.shape[0]\n","          samples_in_object = obj.shape[1]\n","\n","          stand_attrs = [] # ПРИЗНАКИ\n","\n","          # АМПЛИТУДНЫЕ\n","\n","          # все средние по трассам (obj размером 50x100)\n","          all_means = np.mean( np.abs(obj), axis=1 ) # ПРИЗНАКИ\n","          all_stds = np.std( obj, axis=1 ) # ПРИЗНАКИ\n","          mean = np.mean(all_means) # ПРИЗНАКИ\n","          stand_attrs.append(mean)\n","          std = np.std(np.std(obj, axis=1)) # ПРИЗНАКИ\n","          stand_attrs.append(std)\n","\n","          # СПЕКТРАЛЬНЫЕ\n","          samples_step = 2\n","          spectr_attrs = [] # ПРИЗНАКИ\n","\n","          meanTrace = np.mean(obj, axis=0)\n","          obj_withMeanTrace = np.vstack((meanTrace, obj)) # первая трасса - средняя (она даёт обычные спектральные атрибуты)\n","          for l, trace in zip(range(obj_withMeanTrace.shape[0]), obj_withMeanTrace):\n","\n","            fourier = np.fft.fft(trace)\n","            fourier = fourier[int(fourier.size/2):]\n","            fourier = fourier[::-1]\n","\n","            f_discr = 1/(samples_step/1000) # частота дискретизации (не забыли, что samples_step в мс)\n","            df = f_discr/samples_in_object # шаг по частоте (ДПФ: число дискретов образа = числу дискретов прообраза)\n","            faxis = np.arange(0, int(round(f_discr/2)), df) # f_discr/2 - частота Найквиста\n","            spectra = 2/(f_discr*samples_in_object) * (abs(fourier))**2 # нормируем квадрат амплитудного спектра\n","\n","            freq_of_max = spectra.argmax() * df\n","            if l == 0:\n","              stand_attrs.append(freq_of_max)\n","            else:\n","              spectr_attrs.append(freq_of_max) # частота максимума спектра # ПРИЗНАКИ\n","            \n","            energSpec = np.trapz(spectra[:spectra.size], x=faxis) # энергия спектра # ПРИЗНАКИ\n","            if l == 0:\n","              stand_attrs.append(energSpec) # энергия спектра\n","            else:\n","              spectr_attrs.append(energSpec)\n","\n","            spW = 0\n","            if spectra.max() != 0:\n","              spW = energSpec / spectra.max() # ширина спектра # ПРИЗНАКИ\n","\n","            if l == 0:\n","              stand_attrs.append(spW) \n","            else:\n","              spectr_attrs.append(spW)\n","\n","            # Здесь считаем центральную частоту по критерию, что энергии от Fmax/2 до неё и от неё до 2*Fmax совпадают\n","            # или, в нашем случае, отличаются друг от друга не более, чем на 5% полной энергии\n","            Edelta = energSpec*0.05\n","\n","            Fc, Er, El = 0, 0, 0\n","            # Проходим от Fmax/2 до 2*Fmax, считая обе энергии на каждом шаге и сравнивая их\n","            for i in range(int(np.argmax(spectra)/2)+1, int(2*np.argmax(spectra))-1):\n","              Fc = i*df\n","              try:\n","                  El = np.trapz(spectra[int(np.argmax(spectra)/2):i], x=np.arange(np.argmax(spectra)/2*df, Fc, df))\n","              except ValueError:\n","                  El = (i - int(np.argmax(spectra)/2))*df*spectra.max()/2\n","              try:\n","                  Er = np.trapz(spectra[i:int(round(2*np.argmax(spectra)))], x=np.arange(Fc+df/2, 2*np.argmax(spectra)*df, df))\n","              except ValueError:\n","                  Er = (int(round(2*np.argmax(spectra))) - i)*df*spectra.max()/2\n","              if abs(Er - El) < Edelta:\n","                  break\n","            \n","            if l == 0:\n","              stand_attrs.append(Fc) # центральная частота # ПРИЗНАКИ\n","            else:\n","              spectr_attrs.append(Fc)\n","            \n","            if energSpec != 0:\n","              K = (El + Er) / energSpec\n","              \n","            if l == 0:\n","              stand_attrs.append(K) # доля важной энергии спектра # ПРИЗНАКИ\n","            else:\n","              spectr_attrs.append(K)\n","          \n","          # КОРРЕЛЯЦИОННЫЕ\n","          acfField = np.array([]) # АКФ для всех трасс\n","          for trace in obj:\n","              temp = np.correlate(trace, trace, mode='full')\n","              temp = temp[int(temp.size/2):]\n","              acfField = np.append(acfField, temp)\n","          acfField = np.reshape(acfField, (traces_in_object, samples_in_object))\n","\n","          acf_params = np.array([]) # ПРИЗНАКИ\n","          for acf_of_trace in acfField:\n","              acf_max = acf_of_trace.max()\n","              # нахождение индексов массива, после которых значение массива меняет знак\n","              acf_zeros = np.diff(np.sign(acf_of_trace))\n","              try:\n","                  maxtomin_index = np.where(acf_zeros == -2)[0][0]\n","                  mintomax_index = np.where(acf_zeros == 2)[0][0]\n","                  acf_fmin = acf_of_trace[0 : mintomax_index + 1].min()\n","                  ind_fmin = acf_of_trace[0 : mintomax_index + 1].argmin()\n","                  x_fall1 = maxtomin_index\n","                  y_fall1 = acf_of_trace[x_fall1]\n","                  x_fall2 = maxtomin_index + 1\n","                  \n","                  #результат очень сильно зависит от способа интерполяции, поэтому вообще не будем интерполировать\n","                  ind_fzero = x_fall2\n","                  acf_params = np.append(acf_params, np.array([2-ind_fmin/ind_fzero,1-(-acf_fmin)/acf_max,((2-ind_fmin/ind_fzero)**2+(1-(-acf_fmin)/acf_max)**2)**0.5,1/(4*ind_fzero*samples_step/1000)]))\n","              except IndexError:\n","                  acf_params = np.append(acf_params, np.array([-1,-1,-1,-1]))\n","\n","          acf_params_reshape = np.reshape(acf_params, (traces_in_object, 4))\n","\n","          acf_params_mean = acf_params_reshape.sum(axis = 0)/traces_in_object\n","\n","          # в конце добавим обычный усреднённый варинат\n","          stand_attrs.append(acf_params_mean[0]) # временная разреженность # ПРИЗНАКИ\n","          stand_attrs.append(acf_params_mean[1]) # динамическая выраженность # ПРИЗНАКИ\n","          stand_attrs.append(acf_params_mean[2]) # индекс полосы пропускания # ПРИЗНАКИ\n","          stand_attrs.append(acf_params_mean[3]) # доминантная частота # ПРИЗНАКИ\n","\n","          # ЗРЕНИЕ МАШИНЫ\n","          # делаем изображение серым (отображаем значения амплитуд в интервал 0-255, приводим полученный массив к типу byte)\n","          ampField_T = obj.T\n","          im = (np.flip(ampField_T, axis=0) - ampField_T.min())/(ampField_T.max() - ampField_T.min())*255\n","          im = 255 - im\n","          im = im.astype(np.uint8)\n","          im = np.flip(im)\n","\n","          # чуть-чуть заблюриваем картинку (чтобы убрать шум)\n","          blur = cv2.GaussianBlur(im, (3,3), 0)\n","\n","          # применяем пороговый фильтр\n","          # (алгоритм Отсу вычисляет оптимальное значение порога по точке, равноудалённой от двух пиков гистограммы,\n","          # соответствующих чёрному и белому цветам)\n","          ret, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n","\n","          # делаем скелетизацию, т.е. истончение белых участков изображения до линий толщиной в 1 пиксель\n","          thresh2 = thresh.copy()\n","          size = np.size(thresh2)\n","          skel = np.zeros(thresh.shape, np.uint8)\n","          element = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))\n","\n","          done = False\n","          while (not done):\n","              eroded = cv2.erode(thresh2, element)\n","              temp = cv2.dilate(eroded, element)\n","              temp = cv2.subtract(thresh2, temp)\n","              skel = cv2.bitwise_or(skel, temp)\n","              thresh2 = eroded.copy()\n","              zeros = size - cv2.countNonZero(thresh2)\n","              if zeros == size:\n","                  done = True\n","\n","          # делаем преобразование Хафа (его оптимизированную версию Probabilistic Hough Transform), которое на выходе даёт нам все найденные прямолинейные отрезки\n","          # skel - подаваемое изображение\n","          # lines - туда запишутся тета и ро каждой задетектированной линии (пишет координаты начала и конца каждой линии)\n","          # 2ой аргумент - разрешение по ро в пикселях\n","          # 3ий - разрешение по тета в радианах\n","          # 4ый - порог детектирования (сколько пересечений синусоид достаточно для признания линии; количество точек в линии, чтобы та задетектировалась)\n","          # 5ый - минимальное количество точек, способных сформировать линию (только для HoughLinesP)\n","          # 6ой - максимальное расстояние между двумя точками, чтобы считать, что эти точки лежат на одной линии (только для HoughLinesP)\n","          linesP = cv2.HoughLinesP(skel, 30, 0.3*np.pi/180, 200, None, 2, 2)\n","\n","          if draw == True:\n","            plt_Hl = np.zeros(shape=[im.shape[0], im.shape[1], 3], dtype=np.uint8) # picture for Hough lines\n","\n","          houghLenghts = []\n","          houghAngles = []\n","          maxHLen = 0\n","          avgHLen = 0\n","          avgHAng = 0\n","          avgHAngW = 0\n","          stdHAng = 0\n","          # для каждого отрезка находим длину и угол и сразу рисуем их на выделенном для них фоне, цветом обозначаем угол\n","          # затем вычисляем среднюю и максимальную длину, а также разброс по углам\n","          if linesP is not None:\n","              for i in range(len(linesP)):\n","\n","                  l = linesP[i][0] # координаты начала и конца очередной линии\n","                  length = ((l[2]-l[0])**2 + (l[3]-l[1])**2)**0.5 # длина очередной линии\n","                  houghLenghts.append(length)\n","\n","                  angle = np.arctan((l[3]-l[1])/(l[2]-l[0]+0.0001)) # угол наклона очередной линии\n","                  houghAngles.append(angle*180/math.pi) # в градусах\n","\n","                  if draw == True:\n","                    #hsv_to_rgb переводит из HSV (координаты: цветовой тон от 0 до 360 градусов (поэтому и делим на math.pi), насыщенность и яркость) в RGB (красный, зеленый, синий)\n","                    #задаём цвет в HSV, потому что сам по себе цвет задаётся только первой координатой\n","                    color = colorsys.hsv_to_rgb((angle+math.pi/2)/math.pi, 1, 1)\n","                    color = tuple([255*x for x in color])\n","\n","                    #изображение, координаты отрезка, цвет, толщина, cv2.LINE_AA - чтобы отрезок был сглажен\n","                    cv2.line(plt_Hl, (l[0], l[1]), (l[2], l[3]), color, 1, cv2.LINE_AA) # система RGB (для matplotlib)\n","\n","              maxHLen = max(houghLenghts)\n","              avgHLen = np.average(houghLenghts)\n","              avgHAng = np.average(houghAngles)\n","              avgHAngW = np.average(houghAngles, weights=np.array(houghLenghts)/max(houghLenghts))\n","              stdHAng = np.std(houghAngles)\n","              if avgHAng != 0:\n","                  avgHAng = 1 / avgHAng\n","              if avgHAngW != 0:\n","                  avgHAngW = 1 / avgHAngW\n","              if stdHAng != 0:\n","                  stdHAng = 1 / stdHAng\n","\n","          stand_attrs.append(maxHLen) # максимальная длина линий Хафа # ПРИЗНАКИ\n","          stand_attrs.append(avgHLen) # средняя длина линий Хафа # ПРИЗНАКИ\n","          stand_attrs.append(avgHAng) # среднее угла линии Хафа (обратная величина) # ПРИЗНАКИ\n","          stand_attrs.append(avgHAngW) # взвешенное среднее угла линии Хафа (обратная величина) # ПРИЗНАКИ\n","          stand_attrs.append(stdHAng) # стандартное отклонение угла линии Хафа (обратная величина) # ПРИЗНАКИ\n","\n","          # ищем контуры\n","          contours, hierarchy = cv2.findContours(skel, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n","\n","          # создаются чёрные фоновые изображения, на которых будем рисовать контуры\n","          plt_Feat = np.zeros(shape=[im.shape[0], im.shape[1]], dtype=np.uint8) # ПРИЗНАКИ\n","          plt_Str = np.zeros(shape=[im.shape[0], im.shape[1], 3], dtype=np.uint8) # for Straightness # ПРИЗНАКИ\n","          if draw == True:\n","            plt_Tr = plt_Str.copy() # for Traceability\n","\n","          lengths = []            # Длины по всем контурам # ПРИЗНАКИ\n","          straightnesses = []     # Спрямлённости # ПРИЗНАКИ\n","          maxCLen = 0\n","          avgCLen = 0\n","          maxStraightness = 0\n","          avgStraightness = 0\n","          if contours is not None:\n","              for cnt in contours:\n","                  # cv2.arcLength - длина подаваемой кривой, False - говорим, что наша кривая незамкнутая\n","                  lengths.append(cv2.arcLength(cnt, False))\n","\n","                  # cv2.boundingRect - the function calculates and returns the minimal up-right bounding rectangle for the specified point set\n","                  cntRect = cv2.boundingRect(cnt)\n","                  # спрямлённость: cntRect[3] - высота прямоугольника, описывающего контур, cntRect[2] - его ширина\n","                  straightnesses.append(1 - cntRect[3]/cntRect[2])\n","\n","                  cv2.drawContours(plt_Feat, [cnt], -1, [255,255,255], 1)\n","\n","                  color1 = colorsys.hsv_to_rgb(0.75 - cntRect[3]/cntRect[2], 1, 1)\n","                  color1 = tuple([255*x for x in color1])\n","                  cv2.drawContours(plt_Str, [cnt], -1, color1, 1) # система RGB (для matplotlib)\n","\n","                  if draw == True:\n","                    color = colorsys.hsv_to_rgb(cv2.arcLength(cnt, False)/traces_in_object*2, 1, 1 - cntRect[3]/cntRect[2])\n","                    color = tuple([255*x for x in color])\n","                    cv2.drawContours(plt_Tr, [cnt], -1, color, 1) # система RGB (для matplotlib)\n","                    \n","\n","              maxCLen = max(lengths) # ПРИЗНАКИ\n","              avgCLen = np.average(lengths) # ПРИЗНАКИ\n","              maxStraightness = max(straightnesses) # ПРИЗНАКИ\n","              avgStraightness = np.average(straightnesses) # ПРИЗНАКИ\n","\n","          stand_attrs.append(maxCLen) # максимальная длина контура # ПРИЗНАКИ\n","          stand_attrs.append(avgCLen) # средняя длина контура # ПРИЗНАКИ\n","          stand_attrs.append(maxStraightness) # максимальная спрямлённость # ПРИЗНАКИ\n","          stand_attrs.append(avgStraightness) # средняя спрямлённость # ПРИЗНАКИ\n","          stand_attrs.append(len(lengths)) # число найденных контуров # ПРИЗНАКИ\n","\n","          if draw == True:\n","            visualize_objects(obj)\n","            plt.show()\n","\n","            fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4, figsize=(6, 24))\n","            \n","            a1 = ax1.imshow(plt_Hl, cmap=\"seismic\") # цветом выделяется угол наклона\n","            ax1.set_title(\"Hough\")\n","\n","            a2 = ax2.imshow(plt_Tr, cmap=\"seismic\") # цветом выделяется длина контура\n","            ax2.set_title(\"Trans\")\n","\n","            a3 = ax3.imshow(plt_Str, cmap=\"seismic\") # цветом выделяется спрямленность контура\n","            ax3.set_title(\"Straight\")\n","\n","            a4 = ax4.imshow(plt_Feat)\n","            ax4.set_title(\"Feat\")\n","\n","            plt.show()\n","\n","            names_attrs = np.array([\"средняя амплитуда\", \"среднеквадратичная амплитуда\", \\\n","            \"частота максимума спектра\", \"энергия спектра\", \"ширина спектра\", \"центральная частота\", \"доля важной энергии спектра\", \\\n","            \"временная разреженность\", \"динамическая выраженность\", \"индекс полосы пропускания\", \"доминантная частота\", \\\n","            \"максимальная длина линий Хафа\", \"средняя длина линий Хафа\", \\\n","            \"среднее угла линии Хафа (обратная величина)\", \"взвешенное среднее угла линии Хафа (обратная величина)\", \"стандартное отклонение угла линии Хафа (обратная величина)\", \\\n","            \"максимальная длина контура\", \"средняя длина контура\", \"максимальная спрямлённость\", \"средняя спрямлённость\", \"число контуров\"])\n","\n","            for_obj_attrs = dict(zip(names_attrs, stand_attrs))\n","\n","            print()\n","            for keys, values in for_obj_attrs.items():\n","              print(keys, ' = ', values)\n","            print()\n","          \n","          sign_feat = []\n","          sign_feat.extend(all_means)\n","          sign_feat.extend(all_stds)\n","          sign_feat.extend(spectr_attrs)\n","          sign_feat.extend(acf_params)\n","\n","          cv_feat = plt_Feat.flatten()\n","          #cv_feat_forConv = plt_Feat # потом можно получить через reshape\n","          \n","          all_feat = []\n","          all_feat.extend(stand_attrs)\n","          all_feat.extend(sign_feat)\n","          all_feat.extend(cv_feat)\n","          \n","          return np.array(all_feat), np.array(plt_Feat)[np.newaxis, :], np.array(plt_Str)[np.newaxis, :]\n","\n","  features2 = np.zeros(5571)\n","  pict_bw = np.zeros((1, 100, 50))\n","  pict_str = np.zeros((1, 100, 50, 3))\n","  for elem in tqdm(objects):\n","    f = feat_of_one_obj(elem)\n","    \n","    features2 = np.vstack((features2, f[0]))\n","    pict_bw = np.vstack((pict_bw, f[1]))\n","    pict_str = np.vstack((pict_str, f[2]))\n","\n","  if save == True:\n","    name_file = \"{0}.npy\".format(var_name)\n","    np.save(way + name_file, features2[1:])\n","    \n","    name_file = \"{0}_bw.npy\".format(var_name)\n","    np.save(way + name_file, pict_bw[1:])\n","\n","    name_file = \"{0}_str.npy\".format(var_name)\n","    np.save(way + name_file, pict_str[1:])\n","\n","  return features2[1:], pict_bw[1:], pict_str[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qV6O_JYGCb7Q","colab_type":"code","colab":{}},"source":["def reduce_pca(var_name, data, dim):\n","# снижает размерность признаков data до dim\n","# var_name - для сохранения\n","\n","  pca = PCA(n_components = dim)\n","  data_pca = pca.fit_transform(data)\n","  \n","  name_file = '{0}.npy'.format(var_name)\n","  np.save(way + name_file, data_pca)\n","\n","  return data_pca"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gy4xvMFCgRs","colab_type":"code","colab":{}},"source":["def reduce_isomap(var_name, data, dim, n_neighbors):\n","# снижает размерность признаков data до dim\n","# n_neighbors - число соседей, которые нужно учитывать для каждой точки\n","# var_name - для сохранения\n","\n","  iso = Isomap(n_components=dim, n_neighbors=n_neighbors, n_jobs=-1, p=1)\n","  data_iso = iso.fit_transform(data)\n","\n","  name_file = '{0}__neighbors={1}.npy'.format(var_name, n_neighbors)\n","  np.save(way + name_file, data_iso)\n","\n","  return data_iso"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oc-2xfKpCgeG","colab_type":"code","colab":{}},"source":["def reduce_autoencoder(var_name, data, dim):\n","# снижает размерность признаков data до dim\n","# var_name - для сохранения\n","\n","  x_train, x_test = train_test_split(data, test_size = 0.15, random_state=32, shuffle = True) # shuffle = True - перемешиваем выборку\n","  \n","  # изменением гиперпараметров (см. Autoencoders for seismic) была подобрана следующая модель:\n","  # encoder\n","  input_attrs = Input(shape = (20,))\n","  x = Dense(19, activation='tanh', bias_initializer='zeros', kernel_initializer='random_uniform')(input_attrs)\n","  x = Dense(17, activation='tanh', bias_initializer='zeros', kernel_initializer='random_uniform')(x)\n","  code = Dense(dim, activation='linear')(x)\n","  \n","  # decoder\n","  input_code = Input(shape = (dim,))\n","  x = Dense(17, activation='tanh', bias_initializer='zeros', kernel_initializer='random_uniform')(input_code)\n","  x = Dense(19, activation='tanh', bias_initializer='zeros', kernel_initializer='random_uniform')(x)\n","  out_attrs = Dense(20, activation='linear')(x)\n","\n","  encoder = Model(input_attrs, code, name=\"encoder\")\n","  decoder = Model(input_code, out_attrs, name=\"decoder\")\n","  d_ae = Model(input_attrs, decoder(encoder(input_attrs)), name=\"autoencoder\")\n","\n","  l_rate = 0.0005 # скорость обучения\n","  d_ae.compile(Adam(l_rate), loss='mse', metrics=['mae']) # две метрики качества модели: средняя квадратичная ошибка (по ней обучается) и средняя абсолютная\n","\n","  # stop training when a monitored quantity has stopped improving\n","  # patience - сколько эпох терпеть не улучшения качества (здесь не уменьшения val_loss)\n","  # restore_best_weights=True - сохранятся веса с лучшим качеством (здесь с минимальным val_loss)\n","  # обычно добавляется как callback в model.fit, но мы используем scikit-learn API; чтобы не было проблем, надо добавить earlyStopping в GridSearchCV_object.fit, причём как-то хитро (см. ниже)\n","  earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n","\n","  epo = 2000 # возможно можно > 2000\n","\n","  hist = d_ae.fit(x_train, # shape[0] у подаваемых объектов должен быть их числом\n","                x_train, \n","                epochs=epo, # в конце каждой эпохи проводится мониторинг функции потерь (val_loss) на validation_data (эпоха - отработать все объекты (случайно выбирается в течение эпохи количество объектов = количеству объектов в тренировочной выборке))\n","                batch_size=70, # сколько объектов будет накоплено (в сумму функционала ошибки) для одного обновления градиента (по умолчанию 32)\n","                shuffle=True, # перемешивать ли тренировочные данные в конце каждой эпохи\n","                verbose=1, # показ progress bar\n","                validation_data=(x_test, x_test), # данные для контроля переобучения\n","                callbacks = [earlyStopping])\n","  \n","  # графики обучения\n","  N = np.arange(0, len(hist.history[\"loss\"]))\n","  fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n","\n","  ax1.plot(N, hist.history[\"loss\"], label=\"train_loss\")\n","  ax1.plot(N, hist.history[\"val_loss\"], label=\"val_loss\")\n","\n","  ax1.set_title(\"Loss\")\n","  ax1.set_xlabel(\"Epoch\")\n","  ax1.set_ylabel(\"MSE\")\n","  ax1.legend()\n","\n","  fig = plt.figure()\n","  ax2.plot(N, hist.history[\"mae\"], label=\"train_metrics\")\n","  ax2.plot(N, hist.history[\"val_mae\"], label=\"val_metrics\")\n","\n","  ax2.set_title(\"Metrics\")\n","  ax2.set_xlabel(\"Epoch\")\n","  ax2.set_ylabel(\"MAE\")\n","  ax2.legend()\n","\n","  print()\n","  print('loss и metrics на тестовых данных', d_ae.evaluate(x_test, x_test))\n","\n","  name_file = '{0}.npy'.format(var_name)\n","  d_ae.save(way + name_file)\n","\n","  data_autoen = d_ae.get_layer(name=\"encoder\").predict(data)\n","\n","  return data_autoen"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHOT_mtPCgny","colab_type":"code","colab":{}},"source":["def draw_samples_2D(data, limx=[0, 0], limy=[0, 0]):\n","# рисуем точки data, limx limy - масштаб по осям\n","\n","  fig, ax = plt.subplots(figsize=(5, 5))\n","  ax.scatter(data[:, 0], data[:, 1], s=0.05, color='black')\n","\n","  #plt.tick_params(axis='both', which='major', labelsize=28)\n","  #plt.show()\n","  if limx[0] != limx[1]:\n","    ax.set_xlim(limx[0], limx[1])\n","  if limy[0] != limy[1]:\n","    ax.set_ylim(limy[0], limy[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Goq4IfQFChBT","colab_type":"code","colab":{}},"source":["def clust_optics(var_name, data, min_samples, xi, min_cluster_size):\n","# кластеризация данных data через optics; параметры см. ниже\n","# var_name - для сохранения\n","\n","  # Unlike DBSCAN, the OPTICS algorithm does not produce a strict cluster partition, \n","  # but an augmented ordering of the database. To produce the cluster partition, you can use OPTICSxi (already in OPTICS method in sklearn), \n","  # which is algorithm that produces a classification based on the output of OPTICS.\n","  # Parametr 'xi' controls directly the number of classes we will obtain.\n","  # cluster_optics_dbscan in sklearn is another way to produce clusters from output of OPTICS.\n","\n","  # по сути главное отличие от dbscan, что optics использует range of epsilons; \n","  # чем меньше epsilon, тем меньше кластеры, тем их больше; увеличивая epsilon, кластеры сливаются (так в dbscan);\n","  # поэтому, если работать с range of epsilons, можно получить иерархическое разбиение на кластеры\n","\n","  optics = OPTICS(\n","  # по сути для построения графика reachability:\n","               min_samples=min_samples, # число соседних точек, которые сделают данную точку корневым объектом (требующихся для образования нового кластера)\n","               max_eps=np.inf, # максимальная окрестность рассматриваемой точки, в которой все попавшие остальные точки считаются соседями рассматриваемой точки\n","               metric='minkowski', # функция измерения расстояния\n","               p=1, # параметр для метрики Минковского (лучшая метрика для многомерных пространств)\n","  # работают с выходом OPTICS, чтобы извлечь кластеры (извлечь из графика reachability):\n","               cluster_method='xi', # метод извлечения кластеров из выхода алгоритма OPTICS (ещё можно через 'dbscan' (см. ниже))\n","               xi=xi, # it is relative decrease in density (для отделения кластеров друг от друга)\n","               min_cluster_size=min_cluster_size) # размер минимального кластера = подаваемая доля от общего числа точек или число точек\n","  \n","  optics.fit(data)\n","\n","\n","  name_file = '{0}__min_samples={1}_xi={2}_min_cluster_size={3}.pkl'.format(var_name, min_samples, xi, min_cluster_size)\n","  with open(way + name_file,'wb') as f:\n","    pickle.dump(optics, f)\n","\n","  return optics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZOJfm4yCwI4","colab_type":"code","colab":{}},"source":["def reachability_plot(optics, labels_cluster_oder=[], limx=[0, 0], limy=[0, 0], title='', obj_num=None):\n","# рисуем график расстояния достижимости, раскрашиваем на нём найденные кластеры, limx limy - масштаб по осям\n","  \n","  labels = optics.labels_\n","\n","  if len(labels_cluster_oder) == 0:\n","    num_clust = np.unique(labels).shape[0]\n","  else:\n","    num_clust = np.unique(labels_cluster_oder).shape[0]\n","\n","  if num_clust > 1000:\n","    print('too many clusters')\n","    return\n","  \n","  space = np.arange(labels.shape[0])\n","\n","  if len(labels_cluster_oder) == 0:\n","    labels_cluster_oder = labels[optics.ordering_]\n","  reachability_cluster_oder = optics.reachability_[optics.ordering_]\n","\n","  colors_array = np.random.rand(num_clust, 3)\n","  colors_tuple = list(map(tuple, colors_array[:]))\n","\n","  fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 6))\n","  for cluster, color in zip(range(0, num_clust), colors_tuple):\n","    x_k = space[labels_cluster_oder == cluster]\n","    r_k = reachability_cluster_oder[labels_cluster_oder == cluster]\n","    ax.scatter(x_k, r_k, c=[color], s=7, alpha=1, marker='1')\n","    #ax.plot(x_k, r_k, c=[color], marker='1')\n","  ax.scatter(space[labels_cluster_oder == -1], reachability_cluster_oder[labels_cluster_oder == -1], c='black', s=0.1, alpha=0.7, marker='2')\n","  if type(obj_num) != type(None):\n","    ax.axvline(obj_num, 0,10, linewidth=1)\n","  #ax.plot(pace[labels_cluster_oder == -1], reachability_cluster_oder[labels_cluster_oder == -1], c='black', alpha=0.07, marker='2')\n","  \n","  plt.title(title)\n","\n","  if limx[0] != limx[1]:\n","    ax.set_xlim(limx[0], limx[1])\n","  if limy[0] != limy[1]:\n","    ax.set_ylim(limy[0], limy[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUcHQQTACzdu","colab_type":"code","colab":{}},"source":["def visualize_clusters(data, labels):\n","# отображает объекты (разрезы) по кластерам; если больше 100 объектов в кластере, то отображает случайные 100 объектов\n","\n","    num_clust = np.unique(labels).shape[0] # метку -1 тоже считаем\n","    \n","    for i in range(-1, num_clust - 1):\n","        \n","        data_in_clust = data[labels == i]\n","        num_in_clust = data_in_clust.shape[0]\n","        \n","        if num_in_clust <= 100:\n","            visualize_objects(data_in_clust, title=\"Cluster {0} with {1} objects\".format(i, num_in_clust))\n","        else:   \n","            ind_for_imshow = np.random.randint(0, num_in_clust, size=100)\n","            visualize_objects(data_in_clust[ind_for_imshow], title=\"Cluster {0} with {1} objects\".format(i, num_in_clust))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sm-RFsMjC3fX","colab_type":"code","colab":{}},"source":["def draw_clusters_2D(data, labels, limx=[0, 0], limy=[0, 0]):\n","# рисует точки data и раскрашивает их по кластерам labels, limx limy - масштаб по осям\n","# title - заголовок графика\n","# labels_cluster_oder - сюда подаются метки объектов, соответствующих участкам графика достижимости без впадин (метки объектов вне кластеров) (см. ячейку ниже)\n","\n","  num_clust = np.unique(labels).shape[0]\n","  if num_clust > 1000:\n","    print('too many clusters')\n","    return\n","\n","  colors_array = np.random.rand(num_clust, 3)\n","  colors_tuple = list(map(tuple, colors_array))\n","\n","  fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n","  for cluster, color in zip(range(0, num_clust), colors_tuple):\n","    cl_data = data[labels == cluster]\n","    ax.scatter(cl_data[:,0], cl_data[:,1], c=[color], s=5)\n","  ax.scatter(data[labels == -1, 0], data[labels == -1, 1], c='black', marker='1', s=0.05)\n","\n","  if limx[0] != limx[1]:\n","    ax.set_xlim(limx[0], limx[1])\n","  if limy[0] != limy[1]:\n","    ax.set_ylim(limy[0], limy[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1x5RisFcAna","colab_type":"code","colab":{}},"source":["def heavy_clusters(optics):\n","# возвращает метки объектов на линиях между минимумами на графике достижимости (т.е. метки объектов вне кластеров) \n","# в порядке их следования по графику достижимости\n","# (как будто эти линии кластеры)\n","\n","  num_clust = np.unique(optics.labels_).shape[0]\n","  labels_cluster_oder = optics.labels_[optics.ordering_]\n","\n","  clust_id = np.arange(num_clust - 1)\n","\n","  cl_limits = [0]\n","  for cl_id in clust_id:\n","    obj_in_cl = np.where(labels_cluster_oder == cl_id)[0] # np.where возвращает tuple с массивом\n","\n","    cl_start = obj_in_cl[0]\n","    cl_limits.append(cl_start)\n","\n","    cl_end = obj_in_cl[len(obj_in_cl) - 1]\n","    cl_limits.append(cl_end)\n","\n","  cl_limits.append(len(labels_cluster_oder)-1)\n","  cl_limits = np.array(cl_limits)\n","\n","  cl_limits_ = cl_limits\n","\n","  labels_new = np.arange(optics.labels_.shape[0])\n","  st = 0\n","  end = 1\n","  cl_id = 0\n","  while end != len(cl_limits):\n","    if cl_id % 2 == 0:\n","      labels_new[cl_limits[st]:cl_limits[end]+1] = int(cl_id/2)\n","    else:\n","      labels_new[cl_limits[st]:cl_limits[end]+1] = -1\n","\n","    cl_id += 1  \n","    st += 1\n","    end += 1\n","    \n","  return labels_new"],"execution_count":null,"outputs":[]}]}